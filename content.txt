// tensor.hpp
#pragma once
#include <vector>
#include <memory>
#include "device.hpp"
#include "buffer.hpp"

namespace numerical
{

    template <typename T>
    class Tensor
    {
    public:
        Tensor(const std::vector<size_t> &shape); //, Device device = Device::CPU);

        T &operator()(const std::vector<size_t> &indices);
        const T &operator()(const std::vector<size_t> &indices) const;

        size_t size() const { return total_size_; }
        const std::vector<size_t> &shape() const { return shape_; }
        // Device device() const { return device_; }

        T *data() { return buffer_->data(); }
        const T *data() const { return buffer_->data(); }

        template <typename... Range>
        Tensor slice(const Range &range) {}

    private:
        std::vector<size_t> shape_,
            strides_;
        size_t total_size_;
        // Device device_;
        std::unique_ptr<Buffer<T>> buffer_;

        void compute_strides();
        size_t compute_offset(const std::vector<size_t> &indices) const;

        Tensor(const std::vector<size_t> &shape,
               const std::vector<size_t> &strides,
               size_t total_size,
               std::unique_ptr<Buffer<T>> &&buffer)
            : shape_(shape),
              strides_(strides),
              total_size_(total_size),
              buffer_(std::move(buffer)) {}
    };

}

// slice.cpp
#pragma once
#include <vector>
#include <memory>
#include "tensor.hpp"
#include "range.hpp"

namespace numerical
{
    template <typename T>
    template <typename... Range>
    Tensor<T> Tensor<T>::slice(const Range &...ranges)
    {
        // TODO: parse ranges into new shape and strides
        std::vector<size_t> new_shape(sizeof...(ranges));
        for (i = 0, i < sizeof...(ranges), ++i)
        {
            new_shape[i] = (range[i].stop - range[i].start) / range[i].step
        }
        std::vector<size_t> new_strides = {/* calculated from strides and ranges */};

        size_t new_size = 1;
        for (auto d : new_shape)
            new_size *= d;

        // Optional: compute offset and pass offset pointer into new Buffer wrapper

        // Share the buffer
        return Tensor<T>(new_shape, new_strides, new_size, std::make_unique<BufferView<T>>(data()));
    }

};

// range.hpp
#pragma once
#include <cstddef>
#include <iostream>

struct Range
{
    size_t start_;
    size_t stop_;
    size_t step_;
    Range(size_t start, size_t stop, size_t step = 1)
        : start_(start), stop_(stop), step_(step) {};
};

// buffer.hpp
#pragma once
#include <vector>
#include <memory>

namespace numerical
{

    template <typename T>
    class Buffer
    {
    public:
        virtual T *data() = 0;
        virtual const T *data() const = 0;
        virtual ~Buffer() = default;
    };

    template <typename T>
    class CpuBuffer : public Buffer<T>
    {
        std::vector<T> storage_;

    public:
        CpuBuffer(size_t size) : storage_(size) {}
        T *data() override { return storage_.data(); }
        const T *data() const override { return storage_.data(); }
    };
}

// tensor.cpp
#include "tensor.hpp"

namespace numerical
{

    // Constructor
    template <typename T>
    Tensor<T>::Tensor(const std::vector<size_t> &shape) //, Device device)
        : shape_(shape)                                 //, device_(device)
    {
        /*
        if (device_.is_gpu)
        {
            if (device_.device_type == DeviceType::CUDA)
            {
                buffer_ = std::make_unique<CudaBuffer<T>>(total_size_, device_.device_id);
            }
            else if (device_.device_type == DeviceType::ROCm)
            {
                buffer_ = std::make_unique<RocmBuffer<T>>(total_size_, device_.device_id);
            }
            else if (device_.device_type == DeviceType::Vulkan)
            {
                buffer_ = std::make_unique<VulkanBuffer<T>>(total_size_, device_.device_id);
            }
            else if (device_.device_type == DeviceType::Metal)
            {
                buffer_ = std::make_unique<MetalBuffer<T>>(total_size_, device_.device_id);
            }
            else
            {
                throw std::runtime_error("Unsupported GPU type!");
            }
        }
        else
        {
            buffer_ = std::make_unique<CpuBuffer<T>>(total_size_);
        }
        */

        // Deploy buffer
        buffer_ = std::make_unique<CpuBuffer<T>>(total_size_);

        // Compute strides
        compute_strides();

        // Compute total size
        total_size_ = 1;
        for (auto s : shape_)
            total_size_ *= s;
    }

    // Compute strides method
    template <typename T>
    void Tensor<T>::compute_strides()
    {
        strides_.resize(shape_.size());
        size_t stride = 1;
        for (int i = shape_.size() - 1; i >= 0; --i)
        {
            strides_[i] = stride;
            stride *= shape_[i];
        }
    }

    // Compute offset method
    template <typename T>
    size_t Tensor<T>::compute_offset(const std::vector<size_t> &indices) const
    {
        size_t offset = 0;
        for (size_t i = 0; i < indices.size(); ++i)
            offset += indices[i] * strides_[i];
        return offset;
    }

    // Overdrive operator() for easy indexing
    template <typename T>
    T &Tensor<T>::operator()(const std::vector<size_t> &indices)
    {
        return buffer_->data()[compute_offset(indices)];
    }

    template <typename T>
    const T &Tensor<T>::operator()(const std::vector<size_t> &indices) const
    {
        return buffer_->data()[compute_offset(indices)];
    }

    // Explicit instantiation
    template class Tensor<float>;
    template class Tensor<double>;

}
